{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {},
   "source": [
    "# Assignment 19 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "##### 1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2 and that the first set of random centroid is 15, 32, and that the second set is 12, 30. ?\n",
    "1. Using the k-means method, create two clusters for each set of centroid described above.\n",
    "2. For each set of centroid values, calculate the SSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdf0f628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters for Centroids (15, 32): {0: array([ 5, 10, 15, 20]), 1: array([25, 30, 35])}\n",
      "SSE for Centroids (15, 32): 175.0\n",
      "Clusters for Centroids (12, 30): {0: array([ 5, 10, 15, 20]), 1: array([25, 30, 35])}\n",
      "SSE for Centroids (12, 30): 175.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Ans\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Given data points\n",
    "data_points = np.array([5, 10, 15, 20, 25, 30, 35]).reshape(-1, 1)\n",
    "\n",
    "# First set of centroids\n",
    "centroids1 = np.array([[15], [32]])\n",
    "kmeans1 = KMeans(n_clusters=2, init=centroids1, n_init=1).fit(data_points)\n",
    "sse1 = kmeans1.inertia_\n",
    "\n",
    "# Second set of centroids\n",
    "centroids2 = np.array([[12], [30]])\n",
    "kmeans2 = KMeans(n_clusters=2, init=centroids2, n_init=1).fit(data_points)\n",
    "sse2 = kmeans2.inertia_\n",
    "\n",
    "# Clusters and SSE for the first set of centroids\n",
    "cluster_labels1 = kmeans1.labels_\n",
    "cluster1_data = {i: data_points[np.where(cluster_labels1 == i)[0]].flatten() for i in range(2)}\n",
    "\n",
    "# Clusters and SSE for the second set of centroids\n",
    "cluster_labels2 = kmeans2.labels_\n",
    "cluster2_data = {i: data_points[np.where(cluster_labels2 == i)[0]].flatten() for i in range(2)}\n",
    "\n",
    "print(\"Clusters for Centroids (15, 32):\", cluster1_data)\n",
    "print(\"SSE for Centroids (15, 32):\", sse1)\n",
    "print(\"Clusters for Centroids (12, 30):\", cluster2_data)\n",
    "print(\"SSE for Centroids (12, 30):\", sse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "##### 2. Describe how the Market Basket Research makes use of association analysis concepts ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c9533f",
   "metadata": {},
   "source": [
    "<u>Ans</u>:\n",
    "Market Basket Analysis utilizes association analysis concepts to uncover patterns and relationships between items purchased together. Here's how this technique leverages association analysis:\n",
    "- <u>Association Rules</u>: Market Basket Analysis uses association rules to identify strong relationships between products based on their co-occurrence in transactions. These rules help in understanding which items tend to appear together frequently in customer purchases.\n",
    "\n",
    "- <u>Support, Confidence, and Lift</u>: Market Basket Analysis calculates metrics like support, confidence, and lift to quantify the strength of associations between items. Support measures the frequency of co-occurrence, confidence assesses the likelihood of one item being bought given the purchase of another, and lift indicates how much more likely items are bought together compared to random chance.\n",
    "\n",
    "- <u>Predictive Analysis</u>: Market Basket Analysis can be predictive, where it predicts future purchases based on past transaction data. This predictive form uses supervised learning algorithms like classification and regression to understand sequential purchases and cross-selling opportunities.\n",
    "\n",
    "- <u>Differential Analysis</u>: Another aspect is differential market basket analysis, which compares purchase histories across different variables like stores, time periods, or customer groups. This analysis helps in uncovering unique patterns in consumer behavior and can lead to tailored marketing strategies.\n",
    "\n",
    "- <u>Algorithm Usage</u>: Common algorithms like Apriori are employed in Market Basket Analysis to identify frequent itemsets and generate association rules. These algorithms help in discovering patterns efficiently as datasets expand.\n",
    "\n",
    "- <u>Real-World Applications</u>: Market Basket Analysis is widely used by retailers, both online and in-store, to suggest related products to customers. For instance, Amazon's \"Frequently bought together\" feature is a practical application of this technique, showcasing how association analysis enhances customer experience and drives sales.\n",
    "\n",
    "By applying association analysis concepts, Market Basket Analysis enables retailers to gain valuable insights into customer purchasing behavior, optimize product placements, and enhance cross-selling strategies based on identified item associations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "##### 3. Give an example of the Apriori algorithm for learning association rules ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff4d939",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "An example of the Apriori algorithm for learning association rules involves identifying frequent itemsets in a dataset to establish associations between items. Here's a simplified illustration:\n",
    "\n",
    "### Example:\n",
    "Consider a dataset representing transactions in a grocery store:\n",
    "- Transaction 1: {bread, milk, eggs}\n",
    "- Transaction 2: {bread, butter}\n",
    "- Transaction 3: {milk, eggs}\n",
    "- Transaction 4: {bread, milk, butter}\n",
    "- Transaction 5: {bread, milk}\n",
    "\n",
    "### Steps using the Apriori Algorithm:\n",
    "1. **Step 1 - Identify Frequent 1-Itemsets (L1):**\n",
    "   - Count the support of individual items:\n",
    "     - {bread}: 4\n",
    "     - {milk}: 4\n",
    "     - {eggs}: 2\n",
    "     - {butter}: 2\n",
    "   - Set a minimum support threshold (e.g., 2) to determine frequent 1-itemsets: {bread, milk}\n",
    "\n",
    "2. **Step 2 - Generate Candidate 2-Itemsets (C2) and Prune:**\n",
    "   - Join frequent 1-itemsets to create candidate 2-itemsets:\n",
    "     - {bread, milk}\n",
    "   - Prune candidate 2-itemsets by checking subsets for frequency: {bread, milk}\n",
    "\n",
    "3. **Step 3 - Identify Frequent 2-Itemsets (L2):**\n",
    "   - Count the support of candidate 2-itemsets:\n",
    "     - {bread, milk}: 3\n",
    "   - Set a minimum support threshold to determine frequent 2-itemsets: {bread, milk}\n",
    "\n",
    "4. **Generate Association Rules:**\n",
    "   - Calculate confidence for rules like {bread} -> {milk}:\n",
    "     - Confidence({bread} -> {milk}) = Support({bread, milk}) / Support({bread})\n",
    "     - Confidence({bread} -> {milk}) = 3 / 4 = 0.75\n",
    "\n",
    "### Resulting Association Rule:\n",
    "- If a customer buys bread, there is a 75% chance they will also buy milk.\n",
    "\n",
    "This example showcases how the Apriori algorithm iteratively identifies frequent itemsets and generates association rules based on the support and confidence thresholds set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "##### 4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric is used to decide when to end the iteration ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fdd889",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "In hierarchical clustering, the distance between clusters is measured using various metrics to determine how similar or dissimilar clusters are. The choice of distance metric impacts the clustering process and when to end the iteration. Here's an explanation of how this metric is used to decide when to end the iteration:\n",
    "\n",
    "### Distance Metrics in Hierarchical Clustering:\n",
    "1. **Single Linkage:** The distance between clusters is defined as the distance between the closest pair of elements, one from each cluster. This metric focuses on the minimum distance between clusters.\n",
    "\n",
    "2. **Complete Linkage:** The distance between clusters is defined as the distance between the farthest pair of elements, one from each cluster. It emphasizes the maximum distance between clusters.\n",
    "\n",
    "3. **Average Linkage:** The distance between clusters is calculated as the average distance between all pairs of elements, one from each cluster. This metric considers the overall average distance between clusters.\n",
    "\n",
    "4. **Centroid Method:** The distance between clusters is the distance between the mean vectors of the clusters. It measures the distance based on the centroids of the clusters.\n",
    "\n",
    "### Ending the Iteration:\n",
    "- **Stopping Criteria:** The choice of distance metric influences when to stop the clustering process. Common stopping criteria include:\n",
    "  - **Distance Threshold:** Iteration stops when the distance between clusters exceeds a predefined threshold. This threshold can be set based on the problem domain or desired cluster granularity.\n",
    "  - **Number of Clusters:** Iteration stops when a specific number of clusters is reached. This criterion is useful when the desired number of clusters is known in advance.\n",
    "  - **Dendrogram Analysis:** Analyzing the dendrogram, a tree-like diagram representing the clustering process, can help determine when to stop based on the structure of the dendrogram.\n",
    "\n",
    "- **Interpretation:** The distance metric guides the merging of clusters, and the decision to end the iteration is based on achieving the desired level of cluster separation or cohesion. It ensures that clusters are distinct enough or similar enough based on the chosen distance measure.\n",
    "\n",
    "By selecting an appropriate distance metric and defining clear stopping criteria, hierarchical clustering can effectively group data points into clusters based on their similarity or dissimilarity, leading to meaningful and interpretable cluster structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "##### 5. In the k-means algorithm, how do you recompute the cluster centroids ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490933f5",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "In the k-means algorithm, recomputing the cluster centroids involves updating the mean values of the data points within each cluster. Here's how the cluster centroids are recalculated in the k-means algorithm:\n",
    "\n",
    "1. **Initial Centroid Selection:**\n",
    "   - Initially, k objects are randomly selected from the dataset to serve as the initial centers for the clusters. These selected objects are known as cluster means or centroids.\n",
    "\n",
    "2. **Cluster Assignment Step:**\n",
    "   - Each data point is assigned to its closest centroid based on the Euclidean distance between the data point and the centroid. This step is known as the \"cluster assignment step.\"\n",
    "\n",
    "3. **Centroid Update Step:**\n",
    "   - After the assignment step, the algorithm computes the new mean value of each cluster. This process involves recalculating the centroid of each cluster by taking the average of all the data points within that cluster. This step is referred to as the \"centroid update.\"\n",
    "\n",
    "4. **Reassignment of Data Points:**\n",
    "   - Once the centroids have been recalculated, every observation is checked again to determine if it might be closer to a different cluster based on the updated centroids. All data points are reassigned to clusters using the updated cluster means.\n",
    "\n",
    "5. **Iteration and Convergence:**\n",
    "   - The cluster assignment and centroid update steps are iteratively repeated until the cluster assignments stop changing. Convergence is achieved when the clusters formed in the current iteration are the same as those obtained in the previous iteration.\n",
    "\n",
    "6. **Stopping Criteria:**\n",
    "   - The algorithm continues to iterate through the cluster assignment and centroid update steps until convergence is reached or until a maximum number of iterations is reached. By default, the R software uses 10 as the default value for the maximum number of iterations.\n",
    "\n",
    "By following these steps, the k-means algorithm iteratively refines the cluster centroids by updating the mean values of the data points within each cluster until a stable set of clusters is obtained where the assignments no longer change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "##### 6. At the start of the clustering exercise, discuss one method for determining the required number of clusters ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04f352d",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "One method for determining the optimal number of clusters at the start of a clustering exercise is by using the \"Elbow Method.\" This technique helps identify the point where the rate of decrease of the sum of squared distances of data points to their assigned cluster centroids (inertia) significantly slows down, resembling an \"elbow\" shape in the plot. The number of clusters corresponding to this point is considered a good choice for the optimal number of clusters.\n",
    "\n",
    "### Steps to Determine the Number of Clusters Using the Elbow Method:\n",
    "1. **Initialize Clustering Algorithm:** Start by initializing the k-means clustering algorithm with a range of possible cluster numbers (k values).\n",
    "   \n",
    "2. **Calculate Inertia:** For each k value, fit the data to the clustering algorithm and calculate the sum of squared distances of samples to their closest cluster center (inertia).\n",
    "\n",
    "3. **Plot the Elbow Curve:** Plot the number of clusters (k) against the corresponding inertia values. As k increases, inertia tends to decrease. Look for the point where the inertia starts decreasing more slowly, forming an elbow-like bend in the plot.\n",
    "\n",
    "4. **Identify the Optimal Number of Clusters:** The optimal number of clusters is typically located at the \"elbow\" point in the plot. This point signifies the value of k where adding more clusters does not significantly reduce inertia, indicating diminishing returns in terms of clustering quality.\n",
    "\n",
    "5. **Decision Making:** Based on the elbow point, choose the number of clusters that balances the trade-off between capturing meaningful patterns in the data (low inertia) and avoiding overfitting by having too many clusters.\n",
    "\n",
    "By applying the Elbow Method, data analysts and data scientists can make an informed decision on the appropriate number of clusters to use in the clustering algorithm, providing a structured approach to determining the optimal clustering configuration for the dataset at the beginning of the clustering process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b649e",
   "metadata": {},
   "source": [
    "##### 7. Discuss the k-means algorithm's advantages and disadvantages ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57946f7a",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "### Advantages and Disadvantages of the K-Means Algorithm:\n",
    "\n",
    "#### Advantages:\n",
    "1. **Simplicity and Ease of Implementation:**\n",
    "   - K-means is straightforward to understand and implement, making it accessible for beginners and quick to apply to various datasets.\n",
    "\n",
    "2. **Scalability to Large Datasets:**\n",
    "   - The algorithm scales well to large datasets, making it efficient for processing substantial amounts of data.\n",
    "\n",
    "3. **Convergence Guarantee:**\n",
    "   - K-means guarantees convergence, ensuring that the algorithm will reach a stable solution.\n",
    "\n",
    "4. **Flexibility and Adaptability:**\n",
    "   - It is flexible in handling different types of data and distance measures, allowing for customization based on the dataset characteristics.\n",
    "\n",
    "5. **Robustness to Outliers:**\n",
    "   - K-means is relatively robust to outliers and noise, producing consistent results even in the presence of noisy data points.\n",
    "\n",
    "6. **Speed and Efficiency:**\n",
    "   - The algorithm is fast and computationally efficient, capable of handling large datasets and being parallelized or distributed.\n",
    "\n",
    "#### Disadvantages:\n",
    "1. **Sensitivity to Initial Centroids:**\n",
    "   - K-means is sensitive to the initial selection of centroids, which can lead to different clustering results based on the starting points.\n",
    "\n",
    "2. **Difficulty in Choosing Optimal K:**\n",
    "   - Determining the optimal number of clusters (k) is challenging and subjective, as different k values can yield varied clustering outcomes.\n",
    "\n",
    "3. **Assumption of Spherical Clusters:**\n",
    "   - K-means assumes that clusters are spherical and have similar variance, limiting its effectiveness for complex or irregularly shaped clusters.\n",
    "\n",
    "4. **Impact of Outliers and Skewed Data:**\n",
    "   - Outliers, skewed features, or correlated variables can affect the clustering results, requiring preprocessing or normalization of the data.\n",
    "\n",
    "5. **Manual Selection of Parameters:**\n",
    "   - The algorithm requires manual selection of parameters like the number of clusters (k) and distance measures, which can be a drawback for users without domain expertise.\n",
    "\n",
    "6. **Limited Handling of Categorical Data:**\n",
    "   - K-means works best with numerical data and struggles with categorical variables, limiting its applicability to certain types of datasets.\n",
    "\n",
    "In conclusion, while the K-means algorithm offers simplicity, efficiency, and scalability, it also comes with challenges related to initializations, cluster shape assumptions, and parameter selection. Understanding these advantages and disadvantages is crucial for effectively applying K-means clustering in data segmentation tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54020cf8",
   "metadata": {},
   "source": [
    "##### 8. Draw a diagram to demonstrate the principle of clustering ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b57f49",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "### Principle of Clustering with Spatial Contiguity Constraint:\n",
    "\n",
    "In clustering, spatial contiguity constraints can be applied to ensure that clusters are formed based on the proximity or spatial relationships between data points. The diagram below illustrates the principle of clustering with spatial contiguity constraint:\n",
    "\n",
    "Principle of Clustering with Spatial Contiguity Constraint\n",
    "\n",
    "#### Steps to Demonstrate the Principle:\n",
    "1. **Draw the Sites on a Map:**\n",
    "   - Begin by plotting the data points or sites on a map, representing the spatial distribution of the data.\n",
    "\n",
    "2. **Link the Sites by a Connection:**\n",
    "   - Establish connections or links between the sites based on their spatial contiguity or proximity. This step ensures that data points that are close to each other are connected.\n",
    "\n",
    "3. **Spatial Contiguity Constraint:**\n",
    "   - Apply the spatial contiguity constraint to guide the clustering process, where data points linked by connections are more likely to be grouped together in the same cluster.\n",
    "\n",
    "By incorporating spatial contiguity constraints, clustering algorithms can leverage the spatial relationships between data points to form more meaningful and coherent clusters based on the underlying spatial structure of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db681d1",
   "metadata": {},
   "source": [
    "##### 9. During your study, you discovered seven findings, which are listed in the data points below. Using the K-means algorithm, you want to build three clusters from these observations. The clusters C1, C2, and C3 have the following findings after the first iteration ?\n",
    "- `C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),  `\n",
    "- `C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,  `\n",
    "- `C3: (5,5) and (9,9) ` \n",
    "\n",
    "What would the cluster centroids be if you were to run a second iteration? What would this clustering's SSE be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd9a5ef",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "To determine the cluster centroids and the Sum of Squared Errors (SSE) after the second iteration of the K-means algorithm with three clusters and the given observations, we need to update the centroids based on the new assignments. Here are the updated cluster centroids and the SSE:\n",
    "\n",
    "### Cluster Centroids after the Second Iteration:\n",
    "- **C1:** (4, 4)\n",
    "- **C2:** (0, 4)\n",
    "- **C3:** (7, 7)\n",
    "\n",
    "### Sum of Squared Errors (SSE):\n",
    "The SSE after the second iteration of the K-means algorithm with the updated centroids would be calculated by summing the squared distances of each point to its respective cluster centroid and then summing these values across all clusters. The SSE provides a measure of how well the data points are grouped around their cluster centroids.\n",
    "\n",
    "The SSE for this clustering after the second iteration would be:\n",
    "$$ SSE = (2-4)^2 + (4-4)^2 + (6-4)^2 + (0-0)^2 + (4-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (0-0)^2 + (5-7)^2 + (9-7)^2 $$\n",
    "\n",
    "$$ SSE = 4 + 0 + 4 + 0 + 16 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + 4 + 4 = 32 $$\n",
    "\n",
    "Therefore, after the second iteration, the cluster centroids would be (4, 4) for C1, (0, 4) for C2, (7, 7) for C3, and the SSE for this clustering would be 32.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b686929",
   "metadata": {},
   "source": [
    "##### 10. In a software project, the team is attempting to determine if software flaws discovered during testing are identical. Based on the text analytics of the defect details, they decided to build 5 clusters of related defects. Any new defect formed after the 5 clusters of defects have been identified must be listed as one of the forms identified by clustering. A simple diagram can be used to explain this process. Assume you have 20 defect data points that are clustered into 5 clusters and you used the k-means algorithm ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3e676",
   "metadata": {},
   "source": [
    "#Ans:\n",
    "### Clustering Defects in Software Testing Using K-Means Algorithm:\n",
    "\n",
    "In a software project, the team aims to identify and group similar software flaws discovered during testing using the k-means algorithm to form 5 clusters of related defects. Any new defect emerging after the initial 5 clusters must be categorized into one of the existing clusters identified through clustering. Here's a simple diagram illustrating this process with 20 defect data points clustered into 5 groups using the k-means algorithm:\n",
    "\n",
    "#### Diagram Explanation:\n",
    "1. **Initial Defect Data Points:**\n",
    "   - Start with 20 defect data points representing different software flaws identified during testing.\n",
    "\n",
    "2. **K-Means Clustering:**\n",
    "   - Apply the k-means algorithm to group these 20 defect data points into 5 clusters based on their similarities.\n",
    "\n",
    "3. **Formation of 5 Clusters:**\n",
    "   - The k-means algorithm partitions the defect data points into 5 distinct clusters, each representing a group of related defects.\n",
    "\n",
    "4. **Cluster Identification:**\n",
    "   - Once the 5 clusters are formed, any new defect that arises post-clustering should be analyzed based on its characteristics and assigned to one of the existing clusters.\n",
    "\n",
    "5. **Continuous Monitoring and Updating:**\n",
    "   - The process involves continuous monitoring of new defects and updating the clusters as needed to ensure that all defects are appropriately categorized.\n",
    "\n",
    "By utilizing the k-means algorithm for defect clustering, software development teams can efficiently organize and manage software flaws, enabling better defect analysis, prioritization, and resolution based on the identified clusters.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
